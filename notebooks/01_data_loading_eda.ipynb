{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac4ed062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw datasets...\n",
      "Datasets loaded successfully.\n",
      "Merging datasets on the 'date' column...\n",
      "Merge complete.\n",
      "Cleaning and preparing data...\n",
      "No missing values found.\n",
      "Converted 'date' column to datetime format.\n",
      "\n",
      "Processing complete!\n",
      "Cleaned and merged data has been saved to: ../data/processed\\cleaned_merged.csv\n",
      "\n",
      "--- First 5 rows of the processed dataset ---\n",
      "        date   region resource_type  usage_cpu  usage_storage  users_active  \\\n",
      "0 2023-01-01  East US            VM         88           1959           470   \n",
      "1 2023-01-01  East US       Storage         92           1595           388   \n",
      "2 2023-01-01  East US     Container         70            621           414   \n",
      "3 2023-01-01  West US            VM         60           1982           287   \n",
      "4 2023-01-01  West US       Storage         85           1371           351   \n",
      "\n",
      "   economic_index  cloud_market_demand  holiday  \n",
      "0          104.97                 0.99        1  \n",
      "1          104.97                 0.99        1  \n",
      "2          104.97                 0.99        1  \n",
      "3          104.97                 0.99        1  \n",
      "4          104.97                 0.99        1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. Define File Paths ---\n",
    "# The '../' tells the script to go up one directory from 'notebooks' to the main project folder.\n",
    "RAW_DATA_DIR = '../data/raw'\n",
    "PROCESSED_DATA_DIR = '../data/processed'\n",
    "\n",
    "AZURE_USAGE_PATH = os.path.join(RAW_DATA_DIR, 'azure_usage.csv')\n",
    "EXTERNAL_FACTORS_PATH = os.path.join(RAW_DATA_DIR, 'external_factors.csv')\n",
    "# Updated the output filename as per your request\n",
    "OUTPUT_PATH = os.path.join(PROCESSED_DATA_DIR, 'cleaned_merged.csv')\n",
    "\n",
    "# --- 2. Load the Raw Datasets ---\n",
    "print(\"Loading raw datasets...\")\n",
    "try:\n",
    "    azure_df = pd.read_csv(AZURE_USAGE_PATH)\n",
    "    factors_df = pd.read_csv(EXTERNAL_FACTORS_PATH)\n",
    "    print(\"Datasets loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Please make sure the raw data files are in the 'data/raw' directory.\")\n",
    "    # Exit the script if files are not found to prevent further errors.\n",
    "    exit()\n",
    "\n",
    "# --- 3. Merge the Datasets ---\n",
    "# We merge the two dataframes on the 'date' column.\n",
    "print(\"Merging datasets on the 'date' column...\")\n",
    "merged_df = pd.merge(azure_df, factors_df, on='date', how='left')\n",
    "print(\"Merge complete.\")\n",
    "\n",
    "# --- 4. Data Cleaning and Preparation ---\n",
    "print(\"Cleaning and preparing data...\")\n",
    "\n",
    "# Check for missing values after the merge\n",
    "if merged_df.isnull().sum().any():\n",
    "    print(\"Missing values found. Filling them with forward fill method.\")\n",
    "    # Forward fill is a good strategy for time-series data\n",
    "    merged_df.fillna(method='ffill', inplace=True)\n",
    "else:\n",
    "    print(\"No missing values found.\")\n",
    "\n",
    "# Convert 'date' column to datetime objects for proper time-series analysis\n",
    "merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
    "print(\"Converted 'date' column to datetime format.\")\n",
    "\n",
    "# --- 5. Save the Processed Data ---\n",
    "# Create the processed directory if it doesn't exist\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Save the final dataframe to a new CSV file in the 'processed' folder\n",
    "merged_df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f\"\\nProcessing complete!\")\n",
    "print(f\"Cleaned and merged data has been saved to: {OUTPUT_PATH}\")\n",
    "\n",
    "# Display the first 5 rows of the final dataset to verify\n",
    "print(\"\\n--- First 5 rows of the processed dataset ---\")\n",
    "print(merged_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
